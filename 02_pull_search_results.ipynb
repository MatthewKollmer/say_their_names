{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pulling Search Results from Chronicling America",
   "id": "f9aba76477f77b99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T18:07:06.025299Z",
     "start_time": "2024-11-25T18:07:05.661628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil"
   ],
   "id": "df1e1b216cde76c8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1) Overview\n",
    "\n",
    "The following steps demonstrate how I've identified digitized newspaper pages with _potential_ lynching reports in Chronicling America.\n",
    "\n",
    "These steps rely on Chronicling America's advanced search. The process involves:\n",
    "\n",
    "1) conducting searches for victim names within the year and year following their murder\n",
    "2) from the search results, pulling the URLs of OCR pages with potential hits\n",
    "3) saving those URLs in new csv files (one csv file per victim name)\n",
    "\n",
    "The results are 3,994 csv files, one per victim. This means there are 3,994 potential lynching victims represented in the data at this point in the process. Across all those csv files, there are 453,050 URLs to digitized pages with potential references to lynchings."
   ],
   "id": "d0dbe6f7797861d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2) Build Chronicling America Search URL",
   "id": "ec8223d1d4b05f81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv('subset_cleaned_combined_lynch_inventories.csv')\n",
    "\n",
    "# a function that constructs a relevant Chronicling America search URL for every row in our lynching inventory\n",
    "def build_chron_am_search(row):\n",
    "    base_url = \"https://chroniclingamerica.loc.gov/search/pages/results/list/\"\n",
    "    date1 = row['year']\n",
    "    date2 = row['year'] + 1\n",
    "    victim = row['victim_name'].replace(' ', '+')\n",
    "\n",
    "    search_url = (f'{base_url}?date1={date1}&date2={date2}&searchType=advanced&language='\n",
    "                  f'&proxdistance=5&rows=1000&ortext=&proxtext=&phrasetext={victim}'\n",
    "                  f'&andtext=&dateFilterType=yearRange&page=1&sort=date')\n",
    "    \n",
    "    return search_url\n",
    "\n",
    "# applying the function\n",
    "df['search_url'] = df.apply(build_chron_am_search, axis=1)\n",
    "\n",
    "# saving the results to our inventory\n",
    "df.to_csv('subset_cleaned_combined_lynch_inventories.csv')"
   ],
   "id": "a2add9a93e47618b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3) Define Scraping Function\n",
    "\n",
    "This scraping function is one I consistently use when I scrape Chronicling America, but please note that it should be updated periodically to account for any changes in Chronicling America's rate limits. For the latest information on these rate limits, visit: [https://www.loc.gov/apis/json-and-yaml/working-within-limits/](https://www.loc.gov/apis/json-and-yaml/working-within-limits/)"
   ],
   "id": "b567f10b47c00f52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T03:23:46.671206Z",
     "start_time": "2024-11-21T03:23:46.668110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the URL pattern that indicates positive search hits in Chronicling America\n",
    "page_pattern = re.compile(r'/lccn/sn\\d+/\\d{4}-\\d{2}-\\d{2}/ed-\\d/seq-\\d+/')\n",
    "\n",
    "# a scraping function that accounts for Chronicling America's rate limits\n",
    "def scrape_carefully(url, retries=3):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                time.sleep(4)  # sleep time may need adjustment based on changes to the rate limits\n",
    "                return response\n",
    "            \n",
    "            elif response.status_code == 429:\n",
    "                print('Received 429 error. Sorry Chron Am! Waiting one hour before retrying.')\n",
    "                time.sleep(3600)  # sleep time may need adjustment based on changes to the rate limits\n",
    "\n",
    "            else:\n",
    "                time.sleep(4)  # sleep time may need adjustment based on changes to the rate limits\n",
    "            \n",
    "        except Exception:\n",
    "            time.sleep(4)  # sleep time may need adjustment based on changes to the rate limits\n",
    "\n",
    "    return None"
   ],
   "id": "8b7695a5df140e70",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4) Pull URLs from Search Hits and Save Them in CSV Files\n",
    "\n",
    "This step will take some time. It was 11 hours of runtime for me. It does not need to be completed in one sitting, though. I added an 'if statement' that checks to see if a victim name has already been pulled from Chron Am. If it has, the code will skip that victim. This means you can run this code at different times, pulling only new victim search results each time, thereby splitting up the whole runtime."
   ],
   "id": "f0eb6f18c0a948ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:38:28.748624Z",
     "start_time": "2024-11-21T03:23:48.226546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# a 'for-loop' with a progress bar that iterates over the rows in our lynching inventory\n",
    "# it uses the data in those rows to construct our csv files per victim\n",
    "for index, row in tqdm(df.iterrows(), desc='Progress thus far', total=len(df)):\n",
    "    search_url = row['search_url']\n",
    "    victim_name = row['victim_name'].replace(' ', '_')\n",
    "    race = row['victim_race']\n",
    "    gender = row['victim_gender']\n",
    "    lynch_date = row['lynch_date']\n",
    "    scrape_results = []\n",
    "\n",
    "    # an 'if statement' that checks to see if a victim csv already exists\n",
    "    # if it does, it skips that victim, saving time\n",
    "    csv_filename = f'name_clusters/{victim_name}.csv'\n",
    "    if os.path.exists(csv_filename):\n",
    "        continue\n",
    "    \n",
    "    # applies the scrape_carefully() function to the search URL for each victim\n",
    "    scrape_content = scrape_carefully(search_url)\n",
    "    \n",
    "    # if the function fails to pull data, it moves along\n",
    "    # this may happen if the function attempts 3 times in a row and only gets error messages\n",
    "    # it may also happen if there are no search results\n",
    "    if scrape_content is None:\n",
    "        continue\n",
    "\n",
    "    # uses BeautifulSoup to parse the html of search results\n",
    "    soup = BeautifulSoup(scrape_content.text, 'html.parser')\n",
    "    \n",
    "    # saves the relevant URL content from searches\n",
    "    results_list = soup.find('ul', class_='results_list')\n",
    "\n",
    "    # if there are no search results, it moves along\n",
    "    if results_list is None:\n",
    "        continue\n",
    "\n",
    "    # uses the regex page pattern to extract URLs for positive hits\n",
    "    matching_links = results_list.find_all('a', href=page_pattern)\n",
    "\n",
    "    # uses the URL patterns to build the URL and CSV file per victim\n",
    "    for link in matching_links:\n",
    "        link_text = link.get_text(strip=True)\n",
    "        match = page_pattern.search(link['href'])\n",
    "        if match:\n",
    "            matched_href = match.group()\n",
    "            link_href = f'https://chroniclingamerica.loc.gov{matched_href}ocr/'\n",
    "            scrape_results.append({'newspaper': link_text, 'url': link_href, 'race': race, 'gender': gender, 'lynch_date': lynch_date})\n",
    "    \n",
    "    # saves the results as a CSV file in the name_clusters directory\n",
    "    if scrape_results:\n",
    "        df_results = pd.DataFrame(scrape_results)\n",
    "        csv_filename = f'name_clusters/{victim_name}.csv'\n",
    "        df_results.to_csv(csv_filename, index=False)"
   ],
   "id": "d4211d69d2f080ba",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  14%|█▍        | 707/4977 [49:28<4:32:19,  3.83s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received 429 error. Waiting for 1 hour before retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 4977/4977 [11:14:40<00:00,  8.13s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 11:14:40.515581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5) Delete Files from Searches with Too Many Hits\n",
    "\n",
    "In this step, I'm considering any searches with 999+ hits as likely full of false positives. To save myself the trouble of iterating over search result pages and then having to filter out more false positives, I'm just removing these CSV files.\n",
    "\n",
    "This resulted in the deletion of the following CSV files:\n",
    "\n",
    "- james_smith.csv: 999 rows\n",
    "- duke_allen.csv: 1000 rows\n",
    "- george_white.csv: 999 rows\n",
    "- dr_james.csv: 1000 rows\n",
    "- a_king.csv: 1000 rows\n",
    "- henry_allen.csv: 1000 rows\n",
    "- allen_nathaniel.csv: 999 rows\n",
    "- clayton_allen.csv: 1000 rows\n",
    "- will_rogers.csv: 1000 rows\n",
    "- t_washington.csv: 999 rows\n",
    "- allen_brooks.csv: 1000 rows\n",
    "- william_morris.csv: 999 rows\n",
    "- john_brown.csv: 1000 rows\n",
    "- george_washington.csv: 1000 rows\n",
    "- will_faulkner.csv: 999 rows\n",
    "- allen_bowen.csv: 1000 rows\n",
    "- john_smith.csv: 1000 rows\n",
    "- william_allen.csv: 1000 rows\n",
    "- john_williams.csv: 1000 rows\n",
    "- henry_smith.csv: 999 rows\n",
    "- john_davis.csv: 1000 rows\n",
    "- allen_cooper.csv: 1000 rows\n",
    "- a_mcclellan.csv: 1000 rows\n",
    "- joseph_a_smith.csv: 999 rows\n",
    "- allen_bolt.csv: 1000 rows\n",
    "- will_lawton.csv: 999 rows"
   ],
   "id": "68b232f6130ffeb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:48:47.387456Z",
     "start_time": "2024-11-21T14:48:45.825521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "directory = 'name_clusters'\n",
    "\n",
    "# a 'for loop' that checks the number of rows in each CSV file\n",
    "# if the rows reach 999 or more, the file is deleted\n",
    "for file_name in os.listdir(directory):\n",
    "\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    if len(df) >= 999:\n",
    "        print(f'Deleting {file_name}: {len(df)} rows')\n",
    "        os.remove(file_path)"
   ],
   "id": "aa9c2862a1b5bf48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting james_smith.csv: 999 rows\n",
      "Deleting duke_allen.csv: 1000 rows\n",
      "Deleting george_white.csv: 999 rows\n",
      "Deleting dr_james.csv: 1000 rows\n",
      "Deleting a_king.csv: 1000 rows\n",
      "Deleting henry_allen.csv: 1000 rows\n",
      "Deleting allen_nathaniel.csv: 999 rows\n",
      "Deleting clayton_allen.csv: 1000 rows\n",
      "Deleting t_washington.csv: 999 rows\n",
      "Deleting allen_brooks.csv: 1000 rows\n",
      "Deleting william_morris.csv: 999 rows\n",
      "Deleting john_brown.csv: 1000 rows\n",
      "Deleting george_washington.csv: 1000 rows\n",
      "Deleting will_faulkner.csv: 999 rows\n",
      "Deleting allen_bowen.csv: 1000 rows\n",
      "Deleting john_smith.csv: 1000 rows\n",
      "Deleting william_allen.csv: 1000 rows\n",
      "Deleting john_williams.csv: 1000 rows\n",
      "Deleting henry_smith.csv: 999 rows\n",
      "Deleting john_davis.csv: 1000 rows\n",
      "Deleting allen_cooper.csv: 1000 rows\n",
      "Deleting a_mcclellan.csv: 1000 rows\n",
      "Deleting joseph_a_smith.csv: 999 rows\n",
      "Deleting allen_bolt.csv: 1000 rows\n",
      "Deleting will_lawton.csv: 999 rows\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6) Review Number of Total Results\n",
    "\n",
    "At this point in the process, the total number of digitized pages from the Chron Am search results is 453,050. But it's important to note that these results are just pages with the victim's name appearing on them. There are many more filtering steps to be completed in order to deduce how many of these results are lynching reports."
   ],
   "id": "cd6b139984235e24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T18:31:28.935380Z",
     "start_time": "2024-11-25T18:31:27.292159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_hits = 0\n",
    "\n",
    "# a 'for loop' that counts the total number of rows across all CSV files\n",
    "for file_name in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    total_hits += len(df)\n",
    "\n",
    "print(f'Total hits: {total_hits}')"
   ],
   "id": "1daaa53a3d65183e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hits: 453050\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7) Break Data Files into Manageable Chunks\n",
    "\n",
    "To make the next part of the process possible (see 03_scrape_search_results.ipynb), I've grouped the CSV files into manageable chunks. These chunks are defined by CSV files that, when added together, contain no more than 5,000 rows."
   ],
   "id": "c050cd9fa5ed3ddf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T18:31:42.040035Z",
     "start_time": "2024-11-25T18:31:40.110775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# I'm setting the max rows per chunk to 5,000\n",
    "# this can (should) be adjusted depending on how many URLs you're comfortable scraping in a sitting\n",
    "max_rows = 5000\n",
    "\n",
    "# I start with w/ a 'for loop' that counts all the CSV files and their row counts\n",
    "csv_data = []\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    row_count = len(pd.read_csv(file_path))\n",
    "    csv_data.append((file_name, row_count))\n",
    "\n",
    "# next I create chunks (subdirectories) and row counts to be iterated over\n",
    "chunk_number = 1\n",
    "current_chunk = os.path.join(directory, f'chunk_{chunk_number}')\n",
    "os.makedirs(current_chunk, exist_ok=True)\n",
    "current_chunk_rows = 0\n",
    "\n",
    "# a 'for loop' that checks the row counts per CSV file and directs the CSV file to the current chunk\n",
    "# but if the current chunk has a row count that exceeds 5,000, the CSV file is placed in a new chunk\n",
    "for file_name, row_count in csv_data:\n",
    "    if current_chunk_rows + row_count > max_rows:\n",
    "        chunk_number += 1\n",
    "        current_chunk = os.path.join(directory, f'chunk_{chunk_number}')\n",
    "        os.makedirs(current_chunk, exist_ok=True)\n",
    "        current_chunk_rows = 0\n",
    "\n",
    "    src_path = os.path.join(directory, file_name)\n",
    "    dest_path = os.path.join(current_chunk, file_name)\n",
    "    shutil.move(src_path, dest_path)\n",
    "    \n",
    "    current_chunk_rows += row_count"
   ],
   "id": "dff6bd1df3479b12",
   "outputs": [],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
