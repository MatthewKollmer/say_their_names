{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scraping URLs of Victim Name Search Hits",
   "id": "ce05e0f32dfa0b9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os"
   ],
   "id": "3f5100fa3d4e39b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1) Overview\n",
    "\n",
    "The following steps showcase how I scraped digitized newspaper pages from Chronicling America that were labelled as containing victim names (see 02_pull_search_results.ipynb). With over 450,000 pages to scrape, this part of the process took a very long time. \n",
    "\n",
    "It must be emphasized that this notebook needs to be run repeatedly over many days. It will not compile all of the data included in this project in one sitting."
   ],
   "id": "3ed8b5d987ac1939"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2) Define Scraping Function\n",
    "\n",
    "This scrape function is set to work within Chron Am's rate limits as they were defined in late 2024. That is, it's sleep times are set to four seconds per request and one hour if somehow you receive a 429 error (the error message Chron Am will send you if you're scraping too much).\n",
    "\n",
    "Before running this function, you should check Chron Am's most recent rate limits: [https://www.loc.gov/apis/json-and-yaml/working-within-limits/](https://www.loc.gov/apis/json-and-yaml/working-within-limits/). The four second delays in this function are based on the \"Newspapers endpoint\" burst limit of 20 requests per 1 minute. In other words, setting time.sleep to 4 means the function pauses requests for four seconds and it becomes impossible to hit the rate limit of 20 requests per minute. It's therefore important to double-check to see if the \"Newspaper endpoint\" rate limit has changed if and when you run this code. If it has, you may need to add more delay time to the time.sleep lines in the function."
   ],
   "id": "80c8ba5478226cce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def scrape_carefully(url, retries=3):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                time.sleep(4)  # sleep time may need adjustment based on changes to the rate limits\n",
    "                return response\n",
    "            \n",
    "            elif response.status_code == 429:\n",
    "                print('Received 429 error. Sorry Chron Am! Waiting one hour before retrying.')\n",
    "                time.sleep(3600)  # sleep time may need adjustment based on changes to the rate limits\n",
    "\n",
    "            else:\n",
    "                time.sleep(4)  # sleep time may need adjustment based on changes to the rate limits\n",
    "            \n",
    "        except Exception:\n",
    "            time.sleep(4)  # sleep time may need adjustment based on changes to the rate limits\n",
    "\n",
    "    return None"
   ],
   "id": "cba56b196ababe3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2) Point to Directory\n",
    "\n",
    "Since this notebook is meant to be run numerous times, you'll need to change the directory each time. In 02_pull_search_results.ipynb, I split the data into manageable chunks of 4,000 - 5,000 search hits. Whenever you run this notebook, you need to change the label (i.e. chunk_1, chunk_2, chunk_50, chunk_96, etc.) so it runs on different chunks of the data each time."
   ],
   "id": "e24557bd0c0f56c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "directory = 'name_clusters/chunk_96'",
   "id": "e944da8707b17d25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3) Run the Scraping Instance\n",
    "\n"
   ],
   "id": "5b4d1dec7b1fe2d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this little loop counts the rows in all the files in the given directory\n",
    "# using this information it's possible to make a progress bar with tqdm\n",
    "total_rows = 0\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    total_rows += len(df)\n",
    "\n",
    "# a loop that iterates over each row in each csv file and pulls the text\n",
    "# from the urls in the 'url' columns. It relies on BeautifulSoup and the\n",
    "# scrape_carefully function defined above. It puts the scraped text into\n",
    "# a new column called 'text'.\n",
    "with tqdm(total=total_rows, desc='Scraping progress', unit='row') as pbar:\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        newspaper_content = []\n",
    "\n",
    "        for url in df['url']:\n",
    "            try:\n",
    "                response = scrape_carefully(url)\n",
    "\n",
    "                if response and response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                    p_tags = soup.find_all('p')\n",
    "                    p_text = ' '.join([tag.get_text(strip=True) for tag in p_tags])\n",
    "\n",
    "                    newspaper_content.append(p_text)\n",
    "\n",
    "                else:\n",
    "                    newspaper_content.append(None)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Error scraping {url}: {e}')\n",
    "                newspaper_content.append(None)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        df['text'] = newspaper_content\n",
    "        df.to_csv(file_path, index=False)"
   ],
   "id": "5cfeda7b2fcc9b8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4) Record Progress and Repeat\n",
    "\n",
    "You can record your progress any way you like. I chose to type the completed chunks every time I finished one.\n",
    "\n",
    "Chunks completed: \n",
    "\n",
    "chunk_1, chunk_2, chunk_3, chunk_4, chunk_5, chunk_6, chunk_7, chunk_8, chunk_9, chunk_10, chunk_11, chunk_12, chunk_13, chunk_14, chunk_15, chunk_16, chunk_17, chunk_18, chunk_19, chunk_20, chunk_21, chunk_22, chunk_23, chunk_24, chunk_25, chunk_26, chunk_27, chunk_28, chunk_29, chunk_30, chunk_31, chunk_32, chunk_33, chunk_34, chunk_35, chunk_36, chunk_37, chunk_38, chunk_39, chunk_40, chunk_41, chunk_42, chunk_43, chunk_44, chunk_45, chunk_46, chunk_47, chunk_48, chunk_49, chunk_50, chunk_51, chunk_52, chunk_53, chunk_54, chunk_55, chunk_56, chunk_57, chunk_58, chunk_59, chunk_60, chunk_61, chunk_62, chunk_63, chunk_64, chunk_65, chunk_66, chunk_67, chunk_68, chunk_69, chunk_70, chunk_71, chunk_72, chunk_73, chunk_74, chunk_75, chunk_76, chunk_77, chunk_78, chunk_79, chunk_80, chunk_81, chunk_82, chunk_83, chunk_84, chunk_85, chunk_86, chunk_87, chunk_88, chunk_89, chunk_90, chunk_91, chunk_92, chunk_93, chunk_94, chunk_95, chunk_96"
   ],
   "id": "814080a4b92e568"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6450d69c0f1de7a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
