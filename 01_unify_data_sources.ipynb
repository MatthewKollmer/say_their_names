{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Unifying Lynching Inventory Datasets and Subsetting for Our Purposes",
   "id": "36630849a891fa81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "import folium\n",
    "import time"
   ],
   "id": "151e0be6685b1bfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1) Overview\n",
    "\n",
    "The following steps outline everything I've done to build our lynching inventory. \n",
    "\n",
    "I compiled our inventory from two other well-known inventories: the Seguin & Rigby Dataset and the Tolnay-Beck Inventory. I preprocessed both of these data sources then concatenated them. I also subset by the date range relevant to the Virality of Racial Terror project (1865-1921) and by victim's race (either Black or white). I also only included cases where we knew the victim's full name, and the month and general location of their murder.\n",
    "\n",
    "The result is a lynching inventory recording 4,977 victims. Of that number, 1,210 are white and 3,767 are Black. 4,853 are listed as male. 124 are listed as female. This data can be reviewed in the csv file called subset_cleaned_combined_lynch_inventories.csv.\n",
    "\n",
    "To learn more about the original source data, visit these links:\n",
    "\n",
    "- Seguin & Rigby Dataset: [https://archive.ciser.cornell.edu/studies/2833/data-and-documentation](https://archive.ciser.cornell.edu/studies/2833/data-and-documentation)\n",
    "- Seguin & Rigby Project Description: [https://journals.sagepub.com/doi/pdf/10.1177/2378023119841780](https://journals.sagepub.com/doi/pdf/10.1177/2378023119841780)\n",
    "- Tolnay-Beck Inventory Request Form: [https://sites.uw.edu/lynching/contact/](https://sites.uw.edu/lynching/contact/)\n",
    "- Tolnay-Beck Project Description: [https://www.press.uillinois.edu/books/?id=p064135](https://www.press.uillinois.edu/books/?id=p064135)"
   ],
   "id": "33da2905d3fa6f38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2) Preprocessing the Seguin Rigby Dataset\n",
   "id": "6fd01a8e0d02d137"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "segrig_df = pd.read_csv('seguin_rigby_lynching_data.csv')\n",
    "\n",
    "# turning 0 values in the day column to 1\n",
    "segrig_df['day'] = segrig_df['day'].replace(0, 1)\n",
    "\n",
    "# creating 'lynch_date' column with year, month, and date\n",
    "segrig_df['lynch_date'] = pd.to_datetime(segrig_df[['year', 'month', 'day']], errors='coerce')\n",
    "\n",
    "# replacing '.' values in 'city' column with NaN\n",
    "segrig_df['city'] = segrig_df['city'].replace('.', np.nan)\n",
    "\n",
    "# adding 'county' or 'parish' string to county column values (county for all states except Louisiana)\n",
    "segrig_df['county'] = segrig_df.apply(lambda row: row['county'] + ' parish' if row['state'] == 'la' else row['county'] + ' county', axis=1)\n",
    "\n",
    "# creating a lynch_location column from city, county, and state columns\n",
    "segrig_df['lynch_location'] = segrig_df.apply(lambda row: ', '.join([str(val) for val in [row['city'], row['county'], row['state']] if pd.notna(val)]),axis=1)\n",
    "\n",
    "# removing rows where the victim name is only one word\n",
    "segrig_df = segrig_df[segrig_df['victim'].str.split().str.len() > 1]\n",
    "\n",
    "# removing all punctuation from victim names\n",
    "segrig_df['victim'] = segrig_df['victim'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# removing all punctuation from victim gender\n",
    "segrig_df['gender'] = segrig_df['gender'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# retaining only white and Black victims\n",
    "segrig_df = segrig_df[segrig_df['race'].isin(['Black', 'White'])]\n",
    "\n",
    "# adding a source column to label these rows as coming from the Seguin & Rigby dataset\n",
    "segrig_df['source'] = 'seguin-rigby'\n",
    "\n",
    "# changing the column names to more easily concatenate the datasets\n",
    "segrig_df = segrig_df.rename(columns={'victim': 'victim_name'})\n",
    "segrig_df = segrig_df.rename(columns={'race': 'victim_race'})\n",
    "segrig_df = segrig_df.rename(columns={'gender': 'victim_gender'})\n",
    "\n",
    "# dropping all columns except those necessary for our project\n",
    "segrig_df = segrig_df.loc[:, ['victim_name', 'victim_race', 'victim_gender', 'lynch_date', 'year', 'lynch_location', 'source']]\n",
    "\n",
    "# lowercasing all the strings in the dataset\n",
    "segrig_df = segrig_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)"
   ],
   "id": "c51e23500c8c6973",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reviewing the preprocessing results\n",
    "segrig_df"
   ],
   "id": "c45ec91016b44ccf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3) Preprocessing the Tolnay-Beck Inventory",
   "id": "8db507585be230b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tolbec_df = pd.read_excel('tolnay_beck_data_2022.xls')\n",
    "\n",
    "# turning 0 values in the day column to 1\n",
    "tolbec_df['Day'] = tolbec_df['Day'].replace(0, 1)\n",
    "\n",
    "# creating 'lynch_date' column with year, month, and date\n",
    "tolbec_df['lynch_date'] = pd.to_datetime(tolbec_df[['Year', 'Month', 'Day']], errors='coerce')\n",
    "\n",
    "# removing 'Near' and 'Possibly' strings from the 'Place' column\n",
    "tolbec_df['Place'] = tolbec_df['Place'].str.replace(r'\\b(Near|Possibly)\\b', '', regex=True).str.strip()\n",
    "\n",
    "# adding 'county' or 'parish' string to county column values (county for all states except Louisiana)\n",
    "tolbec_df['Lynch County'] = tolbec_df.apply(lambda row: row['Lynch County'] + ' parish' if row['state'] == 'la' else row['Lynch County'] + ' county', axis=1)\n",
    "\n",
    "# creating a lynch_location column from Place, Lynch County, and Lynch State columns\n",
    "tolbec_df['lynch_location'] = tolbec_df.apply(lambda row: ', '.join([str(val) for val in [row['Place'], row['Lynch County'], row['Lynch State']] if pd.notna(val)]),axis=1)\n",
    "\n",
    "# removing rows with unnamed victims\n",
    "tolbec_df = tolbec_df[~tolbec_df['Name'].str.contains('Unnamed', na=False)]\n",
    "\n",
    "# removing all punctuation from victim names\n",
    "tolbec_df['Name'] = tolbec_df['Name'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# removing rows where the victim name is only one word\n",
    "tolbec_df = tolbec_df[tolbec_df['Name'].str.split().str.len() > 1]\n",
    "\n",
    "# removing all punctuation from victim gender\n",
    "tolbec_df['Victim\\'s Sex'] = tolbec_df['Victim\\'s Sex'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# removing hyphenated race labels from victim's race\n",
    "tolbec_df['Victim\\'s Race'] = tolbec_df['Victim\\'s Race'].str.replace(r'-.*', '', regex=True)\n",
    "\n",
    "# retaining only white and Black victims\n",
    "tolbec_df = tolbec_df[tolbec_df['Victim\\'s Race'].isin(['Black', 'White'])]\n",
    "\n",
    "# adding a source column to label these rows as coming from the Tolnay-Beck inventory\n",
    "tolbec_df['source'] = 'tolnay-beck'\n",
    "\n",
    "# changing the column names to more easily concatenate the datasets\n",
    "tolbec_df = tolbec_df.rename(columns={'Name': 'victim_name'})\n",
    "tolbec_df = tolbec_df.rename(columns={'Victim\\'s Race': 'victim_race'})\n",
    "tolbec_df = tolbec_df.rename(columns={'Victim\\'s Sex': 'victim_gender'})\n",
    "tolbec_df = tolbec_df.rename(columns={'Year': 'year'})\n",
    "\n",
    "# dropping all columns except those necessary for our project\n",
    "tolbec_df = tolbec_df.loc[:, ['victim_name', 'victim_race', 'victim_gender', 'lynch_date', 'year', 'lynch_location', 'source']]\n",
    "\n",
    "# lowercasing all the strings in the dataset\n",
    "tolbec_df = tolbec_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)"
   ],
   "id": "9e957848dc712aee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reviewing the preprocessing results\n",
    "tolbec_df"
   ],
   "id": "eec7b331c783249d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4) Concatenating the Datasets",
   "id": "c598db6fa3dc4e70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# combining into a new dataframe called master_df\n",
    "master_df = pd.concat([segrig_df, tolbec_df], ignore_index=True)\n",
    "\n",
    "# subsetting by the year range of the VRT project (1865 to 1921)\n",
    "master_df = master_df[(master_df['year'] >= 1865) & (master_df['year'] <= 1921)]\n",
    "\n",
    "# sorting by year\n",
    "master_df = master_df.sort_values(by='year', ascending=True)\n",
    "\n",
    "# removing duplicates that have the same victim name and year\n",
    "master_df = master_df.drop_duplicates(subset=['victim_name', 'year'], keep='first')\n",
    "\n",
    "# resetting the indices\n",
    "master_df = master_df.reset_index(drop=True)\n",
    "\n",
    "# reviewing the results\n",
    "master_df"
   ],
   "id": "de99f64e46f4df8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# saving the results\n",
    "master_df.to_csv('subset_cleaned_combined_lynch_inventories.csv')"
   ],
   "id": "2a61dbaf7691a193",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5) Cleaning the Data by Hand\n",
    "\n",
    "There is no code at this step. With the master dataframe compiled, I then opened it in Microsoft Excel and reviewed it carefully. I quickly noticed two columns with prolific issues: victim_name and lynch_location. \n",
    "\n",
    "The victim_name column had issues originating from two idiosyncrasies in the source data:\n",
    "- rows where any aliases were included with the victim's name (for example, \"john doe / joe doe\" or \"john 'johnny' doe\") \n",
    "- rows where the victim's name was not actually mentioned but instead their relationship to some other victim was recorded (for example, \"brother of john doe\" or \"mrs john doe\")\n",
    "In both these cases, I hand corrected or deleted any rows with these issues.\n",
    "\n",
    "The lynch_location column had issues originating from the Tolnay-Beck Inventory's Place column. That is, this Place column recorded more than the city or town. It included things like \"near jail in town\" or \"outskirts of\" or \"swamp 5 miles from town.\" Basically, Tolnay-Beck were directionally specific in their place recordings, but they did not separate the town or city location in its own column. I therefore had to correct the rows in my lynch_location column to remove any directional cues since they would make mapping the data more difficult."
   ],
   "id": "2a389d38a401673c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6) Further Preprocessing the Unified Data",
   "id": "633c247a637c44b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "master_df = pd.read_csv('subset_cleaned_combined_lynch_inventories.csv')",
   "id": "bdfa54e859733840",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# dropping 'Unnamed' column of old indices\n",
    "master_df = master_df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# creating a dictionary of state abbreviations and full names\n",
    "us_states = {'al': 'alabama', 'ak': 'alaska', 'az': 'arizona', 'ar': 'arkansas', 'ca': 'california', 'co': 'colorado', 'ct': 'connecticut', 'de': 'delaware', 'fl': 'florida', 'ga': 'georgia', 'hi': 'hawaii', 'id': 'idaho', 'il': 'illinois', 'in': 'indiana', 'ia': 'iowa', 'ks': 'kansas', 'ky': 'kentucky', 'la': 'louisiana', 'me': 'maine', 'md': 'maryland', 'ma': 'massachusetts', 'mi': 'michigan', 'mn': 'minnesota', 'ms': 'mississippi', 'mo': 'missouri', 'mt': 'montana', 'ne': 'nebraska', 'nv': 'nevada', 'nh': 'new hampshire', 'nj': 'new jersey', 'nm': 'new mexico', 'ny': 'new york', 'nc': 'north carolina', 'nd': 'north dakota', 'oh': 'ohio', 'ok': 'oklahoma', 'or': 'oregon', 'pa': 'pennsylvania', 'ri': 'rhode island', 'sc': 'south carolina', 'sd': 'south dakota', 'tn': 'tennessee', 'tx': 'texas', 'ut': 'utah', 'vt': 'vermont', 'va': 'virginia', 'wa': 'washington', 'wv': 'west virginia', 'wi': 'wisconsin', 'wy': 'wyoming'}\n",
    "\n",
    "# replacing state abbreviations with full names in the lynch_location column\n",
    "master_df['lynch_location'] = master_df['lynch_location'].apply(lambda x: x[:-3] + ' ' + us_states[x[-2:]] if x[-2:] in us_states else x)\n",
    "\n",
    "# loading the Nomatim geolocator\n",
    "geolocator = Nominatim(user_agent='matthew_k')\n",
    "\n",
    "# defining the get_lat_long() function\n",
    "def get_lat_long(place):\n",
    "    try:\n",
    "        location = geolocator.geocode(place)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "# creating shells for latitude and longitude values\n",
    "latitude = []\n",
    "longitude = []\n",
    "\n",
    "# iterating over the lynch_location column to get latitudes and longitudes for each lynching\n",
    "for place in tqdm(master_df['lynch_location'], desc=\"Geocoding locations\"):\n",
    "    lat, lon = get_lat_long(place)\n",
    "    latitude.append(lat)\n",
    "    longitude.append(lon)\n",
    "    time.sleep(1)\n",
    "\n",
    "# adding lat/long data to the dataframe\n",
    "master_df['latitude'] = latitude\n",
    "master_df['longitude'] = longitude"
   ],
   "id": "51d2f118021f4fa9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reviewing the results\n",
    "master_df"
   ],
   "id": "4f2e8b6e6a83e029",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# saving the results\n",
    "master_df.to_csv('subset_cleaned_combined_lynch_inventories.csv')"
   ],
   "id": "3d3e49e1d0ee4823",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7) Mapping the Lynchings to Assess Quality of Lat/Long Data",
   "id": "f1dcab8498591a8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# set map start point to middle of continental USA\n",
    "map_start_point = [39.8283, -98.5795]\n",
    "\n",
    "# function that slightly alters lat/long so there are no overlapping points\n",
    "def tweak_coordinates(lat, lon, index, recorded_locations):\n",
    "    if (lat, lon) in recorded_locations:\n",
    "        offset = 0.0001 * (index % 10)\n",
    "        return lat + offset, lon + offset\n",
    "    else:\n",
    "        recorded_locations.add((lat, lon))\n",
    "        return lat, lon\n",
    " \n",
    "# a place to hold the record of previously recorded locations\n",
    "recorded_locations = set()\n",
    "\n",
    "# create Folium basemap\n",
    "map = folium.Map(location=map_start_point, tiles=\"Cartodb Positron\", zoom_start=4)\n",
    "\n",
    "# iterate over lat/long data to create points on map\n",
    "for index, row in master_df.iterrows():\n",
    "    if np.isnan(row['latitude']) or np.isnan(row['longitude']):\n",
    "        continue\n",
    "\n",
    "    # adjust any overlapping coordinates\n",
    "    lat, lon = tweak_coordinates(row['latitude'], row['longitude'], index, recorded_locations)\n",
    "\n",
    "    # hover over the points and you'll see this info\n",
    "    tooltip = f\"<div style='font-size: 11pt'>{row['victim_name']}</div>\" \\\n",
    "              f\"<div style='font-size: 11pt'>{row['lynch_location']}</div>\" \\\n",
    "              f\"<div style='font-size: 11pt'>{row['year']}</div>\"\n",
    "\n",
    "    # add the dots to the map\n",
    "    folium.Circle(\n",
    "        [lat, lon],\n",
    "        tooltip=tooltip,\n",
    "        color='darkred',\n",
    "        radius=10\n",
    "    ).add_to(map)\n",
    "\n",
    "# display\n",
    "map"
   ],
   "id": "176ab8530834e6e0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
