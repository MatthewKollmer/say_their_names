{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Enrich the Dataset",
   "id": "86538ed8f6d65d1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import nltk"
   ],
   "id": "694184b71f56e434",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1) Overview\n",
    "\n",
    "This notebook enriches the name clusters dataset, allowing for further analysis. It does so in the following ways:\n",
    "\n",
    "- reorganizes the directory (removing subdirectory chunks)\n",
    "- fixes some OCR issues to identify more references to victims\n",
    "- creates 'clippings' of the text around references to victims\n",
    "- counts signal words (i.e., words related to violence and race) that appear near the victim's name\n",
    "\n",
    "These are essentially the preliminary data enrichments (i.e., NLP/text-mining steps) necessary for further analysis. More preprocessing will be required for things like geospatially mapping the data, classifying the data, and so forth. But the steps in this notebook make those things possible."
   ],
   "id": "9ee672229d49e234"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2) Reorganize Directories\n",
    "\n",
    "When I scraped the data from Chronicling America, I broke it into chunks. Those chunks remained in the form of subdirectories. Rather than incorporating these subdirectories in all my code, I decided to simply move all the files to the main directory and delete the subdirectories. This step does nothing more, just moves the files to make things easier in the following steps."
   ],
   "id": "edaccc560bb7adba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# move all csvs to main directory and delete the 'chunk' subdirectories\n",
    "\n",
    "directory = 'name_clusters'\n",
    "\n",
    "for sub in os.listdir(directory):\n",
    "    sub_path = os.path.join(directory, sub)\n",
    "\n",
    "    if os.path.isdir(sub_path):\n",
    "        for file in os.listdir(sub_path):\n",
    "            original_file = os.path.join(sub_path, file)\n",
    "            moved_file = os.path.join(directory, file)\n",
    "            shutil.move(original_file, moved_file)\n",
    "\n",
    "        if not os.listdir(sub_path):\n",
    "            os.rmdir(sub_path)"
   ],
   "id": "e2cb8afdb6436c58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3) Fix OCR Name Variations\n",
    "\n",
    "Chronicling America uses fuzzy matching in its search. This means that search results contain not just victim's names spelled correctly, but also variations and/or similar phrases. This is to account for OCR errors, no doubt. But it also means that lots of the pages I scraped do not technically contain exact matches for the victim names, making further processing more difficult.\n",
    "\n",
    "This step responds to the issue by correcting near-matches for victim's names. I created the function fix_names() to do this. It is a conservative OCR correction algorithm that changes only slight variations in victim names. To learn more about it, check out [fix_names_demo.ipynb](https://github.com/MatthewKollmer/messing-around/blob/main/vrt_work/say_their_names/fix_names_demo.ipynb).\n",
    "\n",
    "But first, I had to add the victim names in a new column called 'victim':"
   ],
   "id": "d44c907cb568b8e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get victim name from the csv filename and put it into a new column 'victim'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    victim_name = os.path.splitext(filename)[0].replace('_', ' ')\n",
    "    file_path = os.path.join(directory, filename)\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['victim'] = str(victim_name)\n",
    "        df['victim'] = df['victim'].astype(str).str.lower()\n",
    "        # just ensuring 'text' column is read as string at this step. This is critical at later steps, and it was easy to add to the loop here.\n",
    "        df['text'] = df['text'].astype(str).str.lower()\n",
    "        df.to_csv(file_path, index=False)\n",
    "        del df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')"
   ],
   "id": "e26b54ec785800f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I also wanted to test how many more hits the fix_names() function was enabling, so first I counted how the number of instances of victim names in the data:",
   "id": "f58cafb2bd927654"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# count instances of victim names in the text before running OCR correction\n",
    "\n",
    "total_rows = 0\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            rows = sum(1 for _ in f) - 1 # subtracting the first row since it's the column names\n",
    "            if rows > 0:\n",
    "                    total_rows += rows\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')\n",
    "\n",
    "victim_counts = {}\n",
    "\n",
    "with tqdm(total=total_rows, desc='Counting progress on total rows', unit='rows') as pbar:\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            for _, row in df.iterrows():\n",
    "                victim_name = row['victim']\n",
    "                text = row['text']\n",
    "                    \n",
    "                if pd.isnull(victim_name) or pd.isnull(text):\n",
    "                    continue\n",
    "\n",
    "                count = text.count(victim_name)\n",
    "                if count > 0:\n",
    "                    victim_counts[victim_name] = victim_counts.get(victim_name, 0) + count\n",
    "            \n",
    "            pbar.update(len(df))\n",
    "            del df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')\n",
    "\n",
    "print(f'Number of times the victim names appear in the data: {sum(victim_counts.values())}')\n",
    "\n",
    "# Number of times the victim names appear in the data: 317458"
   ],
   "id": "6e7ed7397c862f27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The result was 317,458 victim names. Keep in mind, the dataset contains 453,050 pages. Each one is supposed to have at least one instance of a victim's name. That means without OCR correction, about 30% of the pages (or more) were without proper spellings of victim names, making them essentially useless in further analysis.\n",
    "\n",
    "Hence the function fix_names():"
   ],
   "id": "dc1746e5de976c2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# A Function That Corrects Names in Text\n",
    "\n",
    "def fix_names(text, victim_name):\n",
    "    full_name = victim_name.split()\n",
    "    for i in range(len(full_name) - 1):\n",
    "        first_name = full_name[i]\n",
    "        second_name = full_name[i + 1]\n",
    "\n",
    "        if len(first_name) >= 3:\n",
    "            first_variants = [re.escape(first_name)]\n",
    "            for character in range(len(first_name)):\n",
    "                first_variants.append(re.escape(first_name[:character]) + '.' + re.escape(first_name[character+1:]))\n",
    "        else:\n",
    "            first_variants = [re.escape(first_name)]\n",
    "\n",
    "        if len(second_name) >= 3:\n",
    "            second_variants = [re.escape(second_name)]\n",
    "            for character in range(len(second_name)):\n",
    "                second_variants.append(re.escape(second_name[:character]) + '.' + re.escape(second_name[character+1:]))\n",
    "\n",
    "        else:\n",
    "            second_variants = [re.escape(second_name)]\n",
    "\n",
    "        first_pattern = '(?:' + '|'.join(first_variants) + ')'\n",
    "        second_pattern = '(?:' + '|'.join(second_variants) + ')'\n",
    "        \n",
    "        pattern = re.compile(rf'({first_pattern})\\W*({second_pattern})', flags=re.IGNORECASE)\n",
    "        \n",
    "        text = pattern.sub(f' {first_name} {second_name} ', text)\n",
    "\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ],
   "id": "97b4591d46020c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And then running the function on all the data:",
   "id": "3123b4e657503f8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run the OCR correction on the data\n",
    "\n",
    "total_rows = 0\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            rows = sum(1 for _ in f) - 1 # subtracting the first row since it's the column names\n",
    "            if rows > 0:\n",
    "                    total_rows += rows\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')\n",
    "\n",
    "with tqdm(total=total_rows, desc='Counting progress on total rows', unit='rows') as pbar:\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['text'] = df['text'].astype(str)\n",
    "            df['victim'] = df['victim'].astype(str)\n",
    "            df['text'] = df.apply(lambda row: fix_names(row['text'], row['victim']), axis=1)\n",
    "            df.to_csv(file_path, index=False)\n",
    "            pbar.update(len(df))\n",
    "            del df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')"
   ],
   "id": "5324943f2ea51fe3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once complete, I recounted instances of victim names:",
   "id": "2db890e1c4520c25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# count the instances of victim names in the text after OCR correction\n",
    "\n",
    "total_rows = 0\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            rows = sum(1 for _ in f) - 1 # subtracting the first row since it's the column names\n",
    "            if rows > 0:\n",
    "                    total_rows += rows\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')\n",
    "\n",
    "victim_counts = {}\n",
    "\n",
    "with tqdm(total=total_rows, desc='Counting progress on total rows', unit='rows') as pbar:\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            for _, row in df.iterrows():\n",
    "                victim_name = row['victim']\n",
    "                text = row['text']\n",
    "                    \n",
    "                if pd.isnull(victim_name) or pd.isnull(text):\n",
    "                    continue\n",
    "\n",
    "                count = text.count(victim_name)\n",
    "                if count > 0:\n",
    "                    victim_counts[victim_name] = victim_counts.get(victim_name, 0) + count\n",
    "            \n",
    "            pbar.update(len(df))\n",
    "            del df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')\n",
    "\n",
    "print(f'Number of times the victim names appear in the data: {sum(victim_counts.values())}')\n",
    "\n",
    "# Number of times the victim names appear in the data: 423041"
   ],
   "id": "bcc5f879af43ff23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The result was 423,041 instances of victim names in the data. That's a marked improvement (+135,583 instances). Yet it should be noted that this figure does not necessarily mean +135,583 pages since some pages probably have more than one instance of the victim's name. But that being said, this step does a lot to enrich the data. It ensures I can review over 135k more instances of victim names–a figure that elides the conservative nature of the fix_names() function.",
   "id": "f97c175d619518ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4) Get Clippings\n",
    "\n",
    "In this step, I used the function get_clippings() to create a new column in the data. This new column contains the 50 words before the victim's name and the 100 words after the victim's name. This is essentially attempting to recreate the size of a newspaper clipping.\n",
    "\n",
    "This range can easily be adapted. All you need to do is change the number of 'prewords' and 'postwords' in the following code. I chose 50 and 100 respectively, however, because in previous iterations of this project, I noticed that victim names often appear near the beginning of a lynching report (within the first 50 words or so), and in turn, 100 words after the victim's name provides sufficient text to verify if it is in fact a lynching report."
   ],
   "id": "72f0eff2c1c2a6d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_clippings(text, victim_name, prewords=50, postwords=100):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    victim_tokens = nltk.word_tokenize(victim_name)\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(len(text) - len(victim_tokens) + 1):\n",
    "        if text[i:i + len(victim_tokens)] == victim_tokens:\n",
    "            indices.append(i)\n",
    "    \n",
    "    if not indices:\n",
    "        return None\n",
    "    \n",
    "    clippings = []\n",
    "    for index in indices:\n",
    "        start_index = max(0, index - prewords)\n",
    "        end_index = index + len(victim_tokens) + postwords\n",
    "        clipping_words = text[start_index:end_index]\n",
    "        clipping = ' '.join(clipping_words)\n",
    "        clippings.append(clipping)\n",
    "    \n",
    "    clippings = 'END CLIPPING | START CLIPPING'.join(clippings)\n",
    "    \n",
    "    return clippings"
   ],
   "id": "1a7164af0f2aa30a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code runs get_clippings() on the dataset:",
   "id": "78c4e1d12adbeb80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_rows = 0\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            rows = sum(1 for _ in f) - 1 # subtracting the first row since it's the column names\n",
    "            if rows > 0:\n",
    "                    total_rows += rows\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')\n",
    "\n",
    "with tqdm(total=total_rows, desc='Counting progress on total rows', unit='rows') as pbar:\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['text'] = df['text'].astype(str)\n",
    "            df['victim'] = df['victim'].astype(str)\n",
    "            df['clippings'] = df.apply(lambda row: get_clippings(row['text'], row['victim'], prewords=50, postwords=100), axis=1)\n",
    "            df.to_csv(file_path, index=False)\n",
    "            pbar.update(len(df))\n",
    "            del df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')"
   ],
   "id": "4a07912240692400",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5) Identify Signal Words\n",
    "\n",
    "Finally, I created two lexicons to help assist in reviewing the clippings. One is 'violence_signals' and the other is 'racist_signals'. This code counts instances of words in these lexicons in the clippings and saves the counts in new columns.\n",
    "\n",
    "This is just a preliminary step to help filter actual lynching reports from false positives. Basically, these counts allow me to look at clippings that are more likely to be positive hits. It is not a conclusive step. No data has been removed from the dataset on the basis of these word counts."
   ],
   "id": "8399cf14cb5cebbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get signal words for lynching/violence\n",
    "\n",
    "violence_signals = ['lynch', 'mob', 'murder', 'posse', 'hang', 'hung', 'burn', 'shot', 'gun', 'stab', 'cut', 'bayonet', 'bullet', 'drown', 'beat', 'whip', 'assault', 'death', 'jail', 'prison']\n",
    "\n",
    "total_rows = 0\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            rows = sum(1 for _ in f) - 1 # subtracting the first row since it's the column names\n",
    "            if rows > 0:\n",
    "                    total_rows += rows\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')\n",
    "\n",
    "with tqdm(total=total_rows, desc='Counting progress on total rows', unit='rows') as pbar:\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['clippings'] = df['clippings'].astype(str)\n",
    "            df['violence_word_count'] = df['clippings'].apply(lambda text: sum(text.count(word) for word in violence_signals))\n",
    "            df.to_csv(file_path, index=False)\n",
    "            pbar.update(len(df))\n",
    "            del df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')"
   ],
   "id": "1fd1b86baffd80fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get signal words for race\n",
    "\n",
    "racist_signals = ['negro', 'colored', 'black', 'nigger', 'negroid', 'mulatto', 'african', 'coon']\n",
    "\n",
    "total_rows = 0\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            rows = sum(1 for _ in f) - 1 # subtracting the first row since it's the column names\n",
    "            if rows > 0:\n",
    "                    total_rows += rows\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')\n",
    "\n",
    "with tqdm(total=total_rows, desc='Counting progress on total rows', unit='rows') as pbar:\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['clippings'] = df['clippings'].astype(str)\n",
    "            df['racist_word_count'] = df['clippings'].apply(lambda text: sum(text.count(word) for word in racist_signals))\n",
    "            df.to_csv(file_path, index=False)\n",
    "            pbar.update(len(df))\n",
    "            del df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error! {e} issue with {file_path}. Just FYI: this file has been skipped.')"
   ],
   "id": "eaf9e1d2a54373e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
